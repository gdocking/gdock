{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_path = '/Users/rodrigo/repos/gdock/benchmark_data/v1-1-0'\n",
    "data_file = f'{benchmark_path}/benchmark_v1-1-0_noweights.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single structure\n",
    "top_list = [1, 5, 10, 50, 200, 400, 1000]\n",
    "for top in top_list:\n",
    "    d = {'high': 0, 'medium': 0, 'acceptable': 0}\n",
    "    for target in df['target_name'].unique():\n",
    "        subdf = df[df['target_name'] == target].sort_values(by='score')\n",
    "        filtereddf = subdf.drop_duplicates(subset='score')\n",
    "        topdf = filtereddf[:top]\n",
    "        if (topdf['irmsd'] <= 1).sum():\n",
    "            d['high'] += 1\n",
    "        if (topdf['irmsd'] <= 2).sum():\n",
    "            d['medium'] += 1\n",
    "        if (topdf['irmsd'] <= 4).sum():\n",
    "            d['acceptable'] += 1\n",
    "\n",
    "    print('top', top, d)\n",
    "\n",
    "print('total targets', len(df['target_name'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cluster_range = [1, 3, 5, 10]\n",
    "\n",
    "for top in top_cluster_range:\n",
    "    d = {'high': 0, 'medium': 0, 'acceptable': 0, 'incorrect': 0}\n",
    "    for target in df['target_name'].unique():\n",
    "        c_df = df[df['target_name'] == target].sort_values(['score', 'cluster_id'])\n",
    "        c_df = c_df.dropna()\n",
    "        c_ranking = c_df.drop_duplicates(subset='cluster_id', keep='first')\n",
    "        cluster_ranking_dic = dict((i+1, j) for i, j in enumerate(c_ranking['cluster_id']) if i+1 <= top)\n",
    "        \n",
    "        high_l = []\n",
    "        medium_l = []\n",
    "        acceptable_l = []\n",
    "        incorrect_l = []\n",
    "        for cluster in cluster_ranking_dic:\n",
    "            topdf = c_df[c_df['cluster_id'] == cluster][:5]\n",
    "            if (topdf['irmsd'] <= 1).sum():\n",
    "                high_l.append(1)\n",
    "            if (topdf['irmsd'] <= 2).sum():\n",
    "                medium_l.append(1)\n",
    "            if (topdf['irmsd'] <= 4).sum():\n",
    "                acceptable_l.append(1)\n",
    "            elif (topdf['irmsd'] > 4).sum():\n",
    "                incorrect_l.append(1)\n",
    "        \n",
    "        if high_l:\n",
    "            d['high'] += 1\n",
    "        if medium_l:\n",
    "            d['medium'] += 1\n",
    "        if acceptable_l:\n",
    "            d['acceptable'] += 1\n",
    "        elif incorrect_l:\n",
    "            d['incorrect'] += 1\n",
    "    print('top', top, d)\n",
    "            \n",
    "print('total targets', len(df['target_name'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
